# MGloP

Local explainability methods aim to clarify individual predictions of black-box models by identifying influential training samples. However, in some contexts, a more comprehensive understanding of the model’s behavior on a global scale is necessary. Global explainability methods predominantly focus on identifying prototypes, which serve as inherently interpretable surrogate models, such as the Nearest Prototype Classifier, designed to approximate the behavior of black-box models as faithfully as possible. These approaches address the question of ``How?'' the model behaves by interpreting the surrogate behavior. However, they often neglect a more direct explanation that addresses the question of ``Why?'' the model behaves in such a manner.

To bridge this gap, we propose InfProto, a method capable of generating a two-fold explanation: (a) sets of prototypes that interpret the model’s behavior (``How?''), and (b) corresponding prototype explanations that elucidate the underlying rationale or reasoning of that behavior in a summarized manner (``Why?''). In other words our prototypes serve as summary of models behavior, and prototype explanation summary of its reasoning for all samples. The experiments show that our prototypes exhibit high faithfulness and the summary of reasoning has a high coverage of reasoning for individual samples, effectively answering both the ``How?'' and the ``Why?'' of global explainability.
