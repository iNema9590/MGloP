# MGloP

Local interpretability methods aim to elucidate individual predictions of black box models by identifying important features or influential training samples. However, there are scenarios where a comprehensive understanding of the model's behavior on a global scale is desired. One way to achieve this is to explain subgroups of data together and by unifying the expalanans of these subgroups to have a brief understanding of the model overall. For this purpose, we propose a framework that finds clusters in the data that can be explained together and generates cluster explanations by summarizing local expalanans. To find such clusters we build a bipartite graph of samples with their local explanans and efficiently create embeddings projecting the explanans nodes to create edges between explanandum. Summarizing local explanations to provide a unified explanation for each cluster relies on the centrality degree of local explanations within the cluster elements. Evaluation of the global explanation's efficiency is conducted using the coverage metric.
