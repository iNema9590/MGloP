{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.transforms as mtrans\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from IF import *\n",
    "from proutils import *\n",
    "from joblib import Parallel, delayed\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from image_explainer import *\n",
    "\n",
    "\n",
    "def surrogate_fidelity(prototypes):\n",
    "    smodel = fit_model(X_test[prototypes], mod_pred[prototypes])\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    smodel.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = smodel(X_test)\n",
    "        predicted = (outputs>0.5).int()\n",
    "        accuracy = (predicted == mod_pred).sum().item() / len(Y_test)\n",
    "    return accuracy\n",
    "dmem=torch.load('data/embeds_DMdog.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d54e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = load_inception_embeds()\n",
    "embeds = torch.load('data/dogfishembeds.pt')\n",
    "\n",
    "X_train = torch.tensor(embeds[\"X_train\"])\n",
    "Y_train = torch.tensor(embeds[\"Y_train\"])\n",
    "\n",
    "X_test = torch.tensor(embeds[\"X_test\"])\n",
    "Y_test = torch.tensor(embeds[\"Y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT = 1e-4\n",
    "class BinClassObjective(BaseObjective):\n",
    "\n",
    "    def train_outputs(self, model, batch):\n",
    "        return model(batch[0])\n",
    "\n",
    "    def train_loss_on_outputs(self, outputs, batch):\n",
    "        return F.binary_cross_entropy(outputs, batch[1])\n",
    "\n",
    "    def train_regularization(self, params):\n",
    "        return L2_WEIGHT * torch.square(params.norm())\n",
    "\n",
    "    def test_loss(self, model, params, batch):\n",
    "        outputs = model(batch[0])\n",
    "        return F.binary_cross_entropy(outputs, batch[1])\n",
    "torch.manual_seed(8)\n",
    "module = LiSSAInfluenceModule(\n",
    "    model=clf,\n",
    "    objective=BinClassObjective(),\n",
    "    train_loader=data.DataLoader(train_set, batch_size=32),\n",
    "    test_loader=data.DataLoader(test_set, batch_size=32),\n",
    "    device=DEVICE,\n",
    "    damp=0.001,\n",
    "    repeat= 1,\n",
    "    depth=1800,\n",
    "    scale= 10,\n",
    ")\n",
    "train_idxs = list(range(X_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89305de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT = 1e-4\n",
    "def fit_model(X, Y):\n",
    "    C = 1 / (X.shape[0] * L2_WEIGHT)\n",
    "    sk_clf = linear_model.LogisticRegression(C=C, tol=1e-8, max_iter=1000)\n",
    "    sk_clf = sk_clf.fit(X.numpy(), Y.numpy())\n",
    "\n",
    "    # recreate model in PyTorch\n",
    "    fc = nn.Linear(768, 1, bias=True)\n",
    "    fc.weight = nn.Parameter(torch.tensor(sk_clf.coef_))\n",
    "    fc.bias = nn.Parameter(torch.tensor(sk_clf.intercept_))\n",
    "\n",
    "    pt_clf = nn.Sequential(\n",
    "        fc,\n",
    "        nn.Flatten(start_dim=-2),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return pt_clf.type('torch.FloatTensor')\n",
    "\n",
    "train_set = data.TensorDataset(X_train, Y_train)\n",
    "test_set = data.TensorDataset(X_test, Y_test)\n",
    "torch.manual_seed(42)\n",
    "clf = fit_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0be9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred = (clf(X_test)>0.5).int()\n",
    "DEVICE= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = clf(X_test)\n",
    "    predicted = (outputs>0.5).int()\n",
    "    accuracy = (predicted == Y_test).sum().item() / len(Y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045bdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idxs = list(range(len(Y_test)))\n",
    "train_idxs = list(range(len(Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute influence for a single test point\n",
    "def compute_influence(test_idx):\n",
    "    # print(f'Compute influence for test point {test_idx}')\n",
    "    influences = module.influences(train_idxs=train_idxs, test_idxs=[test_idx])\n",
    "    return influences.numpy()\n",
    "\n",
    "# Parallel computation of influence scores\n",
    "num_cores = -1\n",
    "\n",
    "influence_scores = Parallel(n_jobs=num_cores)(delayed(compute_influence)(test_idx) for test_idx in tqdm(test_idxs))\n",
    "\n",
    "# Step 7: Save the influence scores to a file\n",
    "influence_scores = np.array(influence_scores)\n",
    "# np.save('influence_scores_dogfish.npy', influence_scores)\n",
    "print('Influence scores saved to influence_scores.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52ce494",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_scores=np.load('data/influence_scores_dogfish.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ecfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(influence_scores, X_train, X_test, 10)\n",
    "G = nx.Graph()\n",
    "for i, embs in enumerate(aide_em):\n",
    "    G.add_node(i, feature=X_test[i].numpy(), bipartite=0)\n",
    "    for ind, influence in embs:\n",
    "        G.add_node(f'ex-{ind}', feature=X_train[ind].numpy(), bipartite=1)\n",
    "        G.add_edge(i, f'ex-{ind}', weight=influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4392a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "def compute_embedding_similarity(graph, embedding_key=\"feature\"):\n",
    "\n",
    "    similarity_dict = {}\n",
    "\n",
    "    # Extract node embeddings\n",
    "    embeddings = {node: graph.nodes[node][embedding_key] for node in graph.nodes}\n",
    "\n",
    "    # Compute pairwise similarity for edges\n",
    "    for u, v in graph.edges():\n",
    "        if u in embeddings and v in embeddings:\n",
    "            similarity = cosine_similarity(\n",
    "                [embeddings[u]], [embeddings[v]]\n",
    "            )[0, 0]\n",
    "            similarity_dict[(u, v)] = similarity\n",
    "        else:\n",
    "            similarity_dict[(u, v)] = 0  # Default if embedding is missing\n",
    "\n",
    "    return similarity_dict\n",
    "\n",
    "\n",
    "def bilouvain_with_embeddings(graph, resolution=2.1, alpha=0):\n",
    "\n",
    "    if not nx.is_bipartite(graph):\n",
    "        raise ValueError(\"Input graph must be bipartite.\")\n",
    "\n",
    "    # Compute embedding similarity\n",
    "    embedding_similarity = compute_embedding_similarity(graph)\n",
    "    min_sim = min(embedding_similarity.values())\n",
    "    if min_sim < 0:\n",
    "        embedding_similarity = {k: v - min_sim for k, v in embedding_similarity.items()}\n",
    "    # Update edge weights\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        topology_weight = data.get(\"weight\", 1)\n",
    "        embedding_weight = embedding_similarity.get((u, v), 0)\n",
    "        combined_weight = alpha * embedding_weight + (1 - alpha) * topology_weight\n",
    "        data[\"weight\"] = combined_weight\n",
    "\n",
    "    partition = community.best_partition(graph, resolution=resolution)\n",
    "\n",
    "    # Convert partition to list of sets\n",
    "    communities = {}\n",
    "    for node, comm_id in partition.items():\n",
    "        communities.setdefault(comm_id, set()).add(node)\n",
    "\n",
    "    return list(communities.values())\n",
    "\n",
    "def get_prototype_bipartite0(graph, communities, bipartite_key=\"bipartite\"):\n",
    "\n",
    "    prototypes = {}\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=0\n",
    "        bipartite_0_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 0]\n",
    "\n",
    "        if not bipartite_0_nodes:\n",
    "            prototypes[i] = None\n",
    "            continue\n",
    "\n",
    "        # Extract embeddings for bipartite=0 nodes\n",
    "        embeddings = {\n",
    "            node: graph.nodes[node][\"feature\"]\n",
    "            for node in bipartite_0_nodes\n",
    "            if \"feature\" in graph.nodes[node]\n",
    "        }\n",
    "\n",
    "        if not embeddings:\n",
    "            prototypes[i] = None\n",
    "            continue\n",
    "\n",
    "        # Compute the centroid of embeddings\n",
    "        centroid = np.mean(list(embeddings.values()), axis=0)\n",
    "\n",
    "        # Find the node closest to the centroid\n",
    "        prototype = min(\n",
    "            embeddings.keys(),\n",
    "            key=lambda node: np.linalg.norm(embeddings[node] - centroid)\n",
    "        )\n",
    "\n",
    "        prototypes[i] = prototype\n",
    "\n",
    "    return prototypes\n",
    "\n",
    "def get_representative_bipartite1(graph, communities, bipartite_key=\"bipartite\", n=10):\n",
    "\n",
    "    representatives = {}\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=1\n",
    "        bipartite_1_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 1]\n",
    "\n",
    "        if not bipartite_1_nodes:\n",
    "            representatives[i] = []\n",
    "            continue\n",
    "\n",
    "        # Compute neighbors of each bipartite=1 node within the community\n",
    "        neighbor_sets = {\n",
    "            node: set(graph.neighbors(node)) & community\n",
    "            for node in bipartite_1_nodes\n",
    "        }\n",
    "\n",
    "        # Initialize coverage and selected nodes\n",
    "        selected_nodes = []\n",
    "        covered_neighbors = set()\n",
    "\n",
    "        for _ in range(min(n, len(bipartite_1_nodes))):\n",
    "            # Select the node that maximizes additional coverage\n",
    "            best_node = max(\n",
    "                bipartite_1_nodes,\n",
    "                key=lambda node: len(neighbor_sets[node] - covered_neighbors)\n",
    "            )\n",
    "\n",
    "            selected_nodes.append(best_node)\n",
    "            covered_neighbors.update(neighbor_sets[best_node])\n",
    "            bipartite_1_nodes.remove(best_node)\n",
    "\n",
    "        # representatives[i] = [list(graph.nodes).index(node) for node in selected_nodes]\n",
    "        representatives[i] = selected_nodes\n",
    "\n",
    "    return representatives\n",
    "\n",
    "def compute_coverage(graph, communities, representatives, bipartite_key=\"bipartite\"):\n",
    "\n",
    "    total_coverage = 0\n",
    "    total_nodes = 0\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=0\n",
    "        bipartite_0_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 0]\n",
    "\n",
    "        if not bipartite_0_nodes or i not in representatives:\n",
    "            continue\n",
    "\n",
    "        # Compute neighbors of representative samples\n",
    "        # rep_nodes = [list(graph.nodes)[index] for index in representatives[i]]\n",
    "        rep_nodes = representatives[i]\n",
    "        rep_neighbors = set(rep_nodes)\n",
    "        # for rep in rep_nodes:\n",
    "        #     rep_neighbors.update(graph.neighbors(rep))\n",
    "\n",
    "        # Compute Jaccard similarity for each bipartite=0 node\n",
    "        for node in bipartite_0_nodes:\n",
    "            node_neighbors = set(graph.neighbors(node))\n",
    "            intersection = len(node_neighbors & rep_neighbors)\n",
    "            union = min(len(rep_neighbors), len(node_neighbors))\n",
    "            # union = len(node_neighbors | rep_neighbors)\n",
    "            total_coverage += intersection / union if union > 0 else 0\n",
    "            total_nodes += 1\n",
    "\n",
    "    return total_coverage / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "\n",
    "def int_clus_sim(communs):\n",
    "    ics=[]\n",
    "    for commun in communs:\n",
    "        ints=[i for i in commun if isinstance(i, int)]\n",
    "        if len(ints)<2:\n",
    "            ics.append(1)\n",
    "        else:\n",
    "            elemnts=X_test[ints]\n",
    "            ics.append(np.mean(cosine_similarity(elemnts)[np.triu_indices(len(elemnts), k=1)]))\n",
    "    return sum(ics)/len(ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b163f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mg_mets():    \n",
    "    mglop=[]\n",
    "    covs=[]\n",
    "    sims=[]\n",
    "    for i in [1.1, 1.3, 0.8, 0.86, 1,2,5]:\n",
    "        communities = bilouvain_with_embeddings(G, resolution=i)\n",
    "        sims.append(int_clus_sim(communities))\n",
    "        prototypes = get_prototype_bipartite0(G, communities)\n",
    "        mglop.append(list(prototypes.values()))\n",
    "        representatives = get_representative_bipartite1(G, communities, n=20)\n",
    "        covs.append(compute_coverage(G, communities, representatives))\n",
    "        \n",
    "    return mglop, covs, sims\n",
    "mglop, covs, sims=mg_mets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(i) for i in mglop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bilouvain_with_embeddings(G, resolution=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a288001",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=[]\n",
    "fids=[]\n",
    "# sim=[]\n",
    "def compute_metrics(i):\n",
    "    acc = nearest_medoid_accuracy(X_test, mod_pred, mglop[i])\n",
    "    fid=surrogate_fidelity(mglop[i])\n",
    "    # labels = cluster_by_prototypes(X_test, mglop[i])\n",
    "    # sim = expected_inter_cluster_similarity(X_test, labels)\n",
    "    \n",
    "    return acc,fid\n",
    "\n",
    "# Parallel computation\n",
    "results = Parallel(n_jobs=2)(delayed(compute_metrics)(i) for i in range(len(mglop)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs,fids = zip(*results)\n",
    "\n",
    "accs=np.array(accs)\n",
    "fids=np.array(fids)\n",
    "sims=np.array(sims)\n",
    "covs=np.array(covs)\n",
    "plt.plot(np.arange(1, 6, 0.5), accs, marker='D', label='Faithfulness')\n",
    "plt.plot(np.arange(1, 6, 0.5), fids, marker='*', label='Fidelity')\n",
    "plt.plot(np.arange(1, 6, 0.5), covs, marker='*', label='Coverage')\n",
    "plt.plot(np.arange(1, 6, 0.5), sims, marker='o', label='Expected similarity')\n",
    "plt.xlabel(\"Resolution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeef91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "\n",
    "N_values=[9, 23, 38, 52, 73, 95]\n",
    "def generate_prototypes(K):\n",
    "    N=N_values[K]\n",
    "    explainer = ProtodashExplainer()\n",
    "    weights, protodash, _ = explainer.explain(X_test, X_test, m=N, kernelType = 'Gaussian')\n",
    "    \n",
    "    return {\n",
    "        \n",
    "        \"dknn\": find_prototypes(X_test.detach(), mod_pred, N),\n",
    "        \"dm\": find_prototypes(dmem, mod_pred, N),\n",
    "        \"protodash\": protodash,\n",
    "        \"mglop\": find_representative_samples(X_test, X_train, influence_scores, N, 20)\n",
    "    }\n",
    "\n",
    "n_jobs = -1 \n",
    "all_protos = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(generate_prototypes)(K)\n",
    "    for K in tqdm(range(len(N_values)))\n",
    ")\n",
    "all_protos_np=np.array(all_protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f93d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60248e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments with MGloP\n",
    "# mglop=Parallel(n_jobs=-1)(delayed(find_representative_samples)(X_test, X_train, influence_scores,i, 20) for i in tqdm(range(10,81,10)))\n",
    "accs=[]\n",
    "fids=[]\n",
    "sil=[]\n",
    "sim=[]\n",
    "def compute_metrics(i):\n",
    "    acc = nearest_medoid_accuracy(X_test, mod_pred, mglop[i])\n",
    "    fid=surrogate_fidelity(mglop[i])\n",
    "    sil = compute_prototype_silhouette_score(X_test, mglop[i])\n",
    "    labels = cluster_by_prototypes(X_test, mglop[i])\n",
    "    sim = expected_inter_cluster_similarity(X_test, labels)\n",
    "    return acc,fid, sil, sim\n",
    "\n",
    "# Parallel computation\n",
    "results = Parallel(n_jobs=64)(delayed(compute_metrics)(i) for i in range(len(mglop)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs,fids, sil, sim = zip(*results)\n",
    "plt.plot(range(10, 81, 10), accs, marker='D', label='Faithfulness')\n",
    "plt.plot(range(10, 81, 10), fids, marker='*', label='Fidelity')\n",
    "plt.plot(range(10, 81, 10), sil, marker='x', label='Silhouette')\n",
    "plt.plot(range(10, 81, 10), sim, marker='o', label='Expected similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f52c9",
   "metadata": {},
   "source": [
    "### Sensetivity Analysis of MGloP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments with MGloP\n",
    "alphas=np.arange(0,1,0.1)\n",
    "mglop=Parallel(n_jobs=-1)(delayed(find_representative_samples)(X_test, X_train, influence_scores,20, 20, alpha=i) for i in tqdm(alphas))\n",
    "accs=[]\n",
    "fids=[]\n",
    "sil=[]\n",
    "sim=[]\n",
    "def compute_metrics(i):\n",
    "    acc = nearest_medoid_accuracy(X_test, mod_pred, mglop[i])\n",
    "    fid=surrogate_fidelity(mglop[i])\n",
    "    sil = compute_prototype_silhouette_score(X_test, mglop[i])\n",
    "    labels = cluster_by_prototypes(X_test, mglop[i])\n",
    "    sim = expected_inter_cluster_similarity(X_test, labels)\n",
    "    return acc,fid, sil, sim\n",
    "\n",
    "# Parallel computation\n",
    "results = Parallel(n_jobs=64)(delayed(compute_metrics)(i) for i in range(len(mglop)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs,fids, sil, sim = zip(*results)\n",
    "plt.plot(alphas, accs, marker='D', label='Faithfulness')\n",
    "plt.plot(alphas, fids, marker='*', label='Fidelity')\n",
    "plt.plot(alphas, sil, marker='x', label='Silhouette')\n",
    "plt.plot(alphas, sim, marker='o', label='Expected similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c378545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity to the number of influential sample to build the graph with\n",
    "mglop=Parallel(n_jobs=-1)(delayed(find_representative_samples)(X_test, X_train, influence_scores,20, i, alpha=0.6) for i in tqdm(range(5,41,5)))\n",
    "accs=[]\n",
    "fids=[]\n",
    "sil=[]\n",
    "sim=[]\n",
    "def compute_metrics(i):\n",
    "    acc = nearest_medoid_accuracy(X_test, mod_pred, mglop[i])\n",
    "    fid=surrogate_fidelity(mglop[i])\n",
    "    sil = compute_prototype_silhouette_score(X_test, mglop[i])\n",
    "    labels = cluster_by_prototypes(X_test, mglop[i])\n",
    "    sim = expected_inter_cluster_similarity(X_test, labels)\n",
    "    return acc,fid, sil, sim\n",
    "\n",
    "# Parallel computation\n",
    "results = Parallel(n_jobs=64)(delayed(compute_metrics)(i) for i in range(len(mglop)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs,fids, sil, sim = zip(*results)\n",
    "plt.plot(range(5,41,5), accs, marker='D', label='Faithfulness')\n",
    "plt.plot(range(5,41,5), fids, marker='*', label='Fidelity')\n",
    "plt.plot(range(5,41,5), sil, marker='x', label='Silhouette')\n",
    "plt.plot(range(5,41,5), sim, marker='o', label='Expected similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce6426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8472332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(influence_scores, X_train, X_test, 10, coverage=True)\n",
    "# prs=find_representative_samples(X_test, X_train, influence_scores, 20, 20, alpha=0.6)\n",
    "\n",
    "\n",
    "def coverage(prots,N=30, bin=False, popularity=True):\n",
    "    distances = cosine_similarity(X_test, X_test[prots])\n",
    "    nearest_medoid_indices = np.argmax(distances, axis=1)\n",
    "    covs=[]\n",
    "    for i in np.unique(nearest_medoid_indices):\n",
    "        idx=np.where(nearest_medoid_indices == i)[0]\n",
    "        if popularity:\n",
    "            gx=[i[0] for i in Counter(np.array(aide_em)[idx].flatten()).most_common(15)]\n",
    "        else:\n",
    "            gx=aide_em[prots[i]]\n",
    "        for j in idx:\n",
    "            if bin:\n",
    "                covs.append(len(set(aide_em[j]).intersection(set(gx)))/len(aide_em[j]))\n",
    "                # if len(set(aide_em[j]).intersection(set(gx)))>N:\n",
    "                #     covs.append(1)\n",
    "                # else:\n",
    "                #     covs.append(0)\n",
    "            else:\n",
    "                covs.append(len(set(aide_em[j]).intersection(set(gx)))*100/len(aide_em[j]))\n",
    "    # return sum(covs)/len(covs)\n",
    "    return covs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df19933",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(i) for i in mglop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde853bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=5\n",
    "mglop_co= coverage(list(all_protos_np[i].values())[3],N=10, bin=True, popularity=True)\n",
    "dknn_co= coverage(list(all_protos_np[i].values())[0],N=10, bin=True, popularity=True)\n",
    "dmodels_co= coverage(list(all_protos_np[i].values())[1],N=10, bin=True, popularity=True)\n",
    "protodash_co= coverage(list(all_protos_np[i].values())[2],N=10, bin=True, popularity=True)\n",
    "\n",
    "\n",
    "data = [mglop_co, dknn_co, protodash_co, dmodels_co]\n",
    "labels = ['InfProto', 'KMEx', 'PD', 'DM']\n",
    "# Prepare data for boxplot\n",
    "boxplot_data = []\n",
    "for method_data in data:\n",
    "    sorted_data = sorted(Counter(method_data).items())\n",
    "    y_values = [value for _, value in sorted_data]\n",
    "\n",
    "    # Convert raw counts to percentages\n",
    "    total_samples = sum(y_values)\n",
    "    percentages = [(value * 100) / total_samples for value in y_values]\n",
    "    \n",
    "    # Expand coverage values based on their frequency\n",
    "    expanded_values = []\n",
    "    for (coverage_val, count) in sorted_data:\n",
    "        expanded_values.extend([coverage_val] * count)\n",
    "    \n",
    "    boxplot_data.append(expanded_values)\n",
    "\n",
    "# Define professional color palette\n",
    "colors = ['#1f77b4', '#ff7f0e',  '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "box = plt.boxplot(boxplot_data, labels=labels, patch_artist=True, notch=True, showmeans=True, medianprops={'color': 'black'})\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "# Apply custom colors to the boxes\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor('black')\n",
    "    patch.set_linewidth(1.5)\n",
    "\n",
    "# Customize whiskers, caps, and fliers\n",
    "for whisker, cap in zip(box['whiskers'], box['caps']):\n",
    "    whisker.set(color='black', linewidth=1.2)\n",
    "    cap.set(color='black', linewidth=1.5)\n",
    "\n",
    "for flier in box['fliers']:\n",
    "    flier.set(marker='o', color='black', alpha=0.7)\n",
    "\n",
    "# Styling\n",
    "plt.ylabel(\"Coverage\", fontsize=40)\n",
    "plt.xticks(fontsize=40)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add legend for better understanding\n",
    "handles = [plt.Line2D([0], [0], color=color, lw=4) for color in colors]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Figures/covbox_dog.pdf\", bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=2\n",
    "mglop_co= coverage(list(all_protos_np[i].values())[3],N=10, bin=True, popularity=True)\n",
    "dknn_co=coverage(list(all_protos_np[i].values())[0],N=10, bin=True, popularity=True)\n",
    "dmodels_co=coverage(list(all_protos_np[i].values())[1],N=10, bin=True, popularity=True)\n",
    "protodash_co=coverage(list(all_protos_np[i].values())[2],N=10, bin=True, popularity=True)\n",
    "\n",
    "data = [mglop_co, dknn_co, dmodels_co, protodash_co]\n",
    "labels = ['InfProto', 'KMEx', 'DM', 'PD']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i,j in zip(data, labels): \n",
    "    sorted_data = sorted(Counter(i).items())\n",
    "    x_values = [key for key, _ in sorted_data]\n",
    "    y_values = [value for _, value in sorted_data]\n",
    "\n",
    "    # Calculate cumulative sum for \"samples with value or more\"\n",
    "    cumulative_sum = np.cumsum(y_values[::-1])[::-1]\n",
    "    total_samples = sum(y_values)\n",
    "    proportions = [c*100 / total_samples for c in cumulative_sum]\n",
    "\n",
    "    # Plot the line chart\n",
    "    plt.plot(proportions, x_values, linewidth=2, label=j)\n",
    "\n",
    "# Customize the plot\n",
    "plt.ylabel(\"Coverage\", fontsize=12)\n",
    "plt.xlabel(\"Percentage of Samples (%)\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(prots,N=30, bin=False, popularity=True):\n",
    "    distances = cosine_similarity(X_test, X_test[prots])\n",
    "    nearest_medoid_indices = np.argmax(distances, axis=1)\n",
    "    covs=[]\n",
    "    for i in np.unique(nearest_medoid_indices):\n",
    "        idx=np.where(nearest_medoid_indices == i)[0]\n",
    "        if popularity:\n",
    "            gx=[i[0] for i in Counter(np.array(aide_em)[idx].flatten()).most_common(20)]\n",
    "        else:\n",
    "            gx=aide_em[prots[i]]\n",
    "        for j in idx:\n",
    "            if bin:\n",
    "                if len(set(aide_em[j]).intersection(set(gx)))>N:\n",
    "                    covs.append(1)\n",
    "                else:\n",
    "                    covs.append(0)\n",
    "            else:\n",
    "                covs.append(len(set(aide_em[j]).intersection(set(gx)))/len(aide_em[j]))\n",
    "    return sum(covs)/len(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOg\n",
    "# all_protos_np=np.array(all_protos)\n",
    "mglop_cov=[]\n",
    "protodash_cov=[]\n",
    "dknn_cov=[]\n",
    "dmodels_cov=[]\n",
    "for i in range(len(all_protos_np)):\n",
    "    mglop_cov.append(coverage(list(all_protos_np[i].values())[3],N=10, bin=False, popularity=True))\n",
    "    dknn_cov.append(coverage(list(all_protos_np[i].values())[0],N=10, bin=False, popularity=True))\n",
    "    dmodels_cov.append(coverage(list(all_protos_np[i].values())[1],N=10, bin=False, popularity=True))\n",
    "    protodash_cov.append(coverage(list(all_protos_np[i].values())[2],N=10, bin=False, popularity=True))\n",
    "   \n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(5,100)\n",
    "plt.plot(N_values, np.array(mglop_cov)+0.1, marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, dknn_cov, marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, protodash_cov, marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, dmodels_cov, marker='D', linestyle='-', label='DM')\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Coverage\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "plt.savefig(\"Figures/cov_dog.pdf\", bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_values=range(10,101, 10)\n",
    "def generate_prototypes(N):\n",
    "    explainer = ProtodashExplainer()\n",
    "    weights, protodash, _ = explainer.explain(X_test, X_test, m=N, kernelType='euclid')\n",
    "    \n",
    "    return {\n",
    "        \"mglop\": find_representative_samples(X_test, X_train, influence_scores, N, 20),\n",
    "        \"dknn\": find_prototypes(X_test, mod_pred, N),\n",
    "        \"dm\": find_prototypes(dmem, mod_pred, N),\n",
    "        \"protodash\": protodash\n",
    "    }\n",
    "\n",
    "n_jobs = -1 \n",
    "all_protos = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(generate_prototypes)(N)\n",
    "    for N in tqdm(N_values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['mglop', 'mglop2', 'dm', 'dknn', 'protodash']\n",
    "N_values = range(10, 101, 10)  # Increasing N values from 10 to 80\n",
    "intersections = {f\"{algo1} ∩ {algo2}\": [] for i, algo1 in enumerate(algorithms) for algo2 in algorithms[i + 1:]}\n",
    "\n",
    "\n",
    "# Extract intersections between algorithm outputs\n",
    "for i, outputs in enumerate(all_protos):\n",
    "    for j, algo1 in enumerate(algorithms):\n",
    "        for algo2 in algorithms[j + 1:]:\n",
    "            intersection_size = len(set(outputs[algo1]) & set(outputs[algo2]))\n",
    "            intersections[f\"{algo1} ∩ {algo2}\"].append(intersection_size)\n",
    "\n",
    "# Plotting the results\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (label, sizes), linestyle, marker in zip(intersections.items(), line_styles, markers):\n",
    "    plt.plot(N_values, sizes, linestyle=linestyle, marker=marker, label=label)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Intersection Size\")\n",
    "plt.title(\"Intersection Size vs Number of Elements (N) for Algorithm Outputs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as a PDF\n",
    "# plt.savefig(\"Figures/intersection_plot.pdf\", format=\"pdf\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ce5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def assign_clusters(X, prototypes):\n",
    "\n",
    "    # Compute distance from each point to each prototype\n",
    "    distances = cdist(X, prototypes, metric='euclidean')\n",
    "    \n",
    "    # Assign each point to the cluster with minimum distance\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    return labels\n",
    "\n",
    "pairwise_aris = {\n",
    "    \"KMEx\": [],\n",
    "    \"DM\": [],\n",
    "    \"PD\": []\n",
    "    # \"DkNN-DM\": [],\n",
    "    # \"DkNN-PDash\": [],\n",
    "    # \"DM-PDash\": []\n",
    "}\n",
    "\n",
    "for i in range(len(all_protos)):\n",
    "    labels1 = assign_clusters(X_test, X_test[list(all_protos[i].values())[0]])\n",
    "    labels2 = assign_clusters(X_test, X_test[list(all_protos[i].values())[1]])\n",
    "    labels3 = assign_clusters(X_test, X_test[list(all_protos[i].values())[2]])\n",
    "    labels4 = assign_clusters(X_test, X_test[list(all_protos[i].values())[3]])\n",
    "    \n",
    "    \n",
    "    # Compute ARIs for all pairs\n",
    "    ari_12 = adjusted_rand_score(labels1, labels2)\n",
    "    ari_13 = adjusted_rand_score(labels1, labels3)\n",
    "    ari_14 = adjusted_rand_score(labels1, labels4)\n",
    "    ari_23 = adjusted_rand_score(labels2, labels3)\n",
    "    ari_24 = adjusted_rand_score(labels2, labels4)\n",
    "    ari_34 = adjusted_rand_score(labels3, labels4)\n",
    "    \n",
    "    # Append results\n",
    "    pairwise_aris[\"KMEx\"].append(ari_12)\n",
    "    pairwise_aris[\"DM\"].append(ari_13)\n",
    "    pairwise_aris[\"PD\"].append(ari_14)\n",
    "    # pairwise_aris[\"DkNN-DM\"].append(ari_23)\n",
    "    # pairwise_aris[\"DkNN-PDash\"].append(ari_24)\n",
    "    # pairwise_aris[\"DM-PDash\"].append(ari_34)\n",
    "\n",
    "# Plot the ARI evolution for each pair\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(5,100)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.plot(N_values, pairwise_aris[\"KMEx\"], marker='s', label='KMEx', color='orange')\n",
    "plt.plot(N_values, pairwise_aris[\"PD\"], marker='^', label='PD', color='green')\n",
    "plt.plot(N_values, pairwise_aris[\"DM\"], marker='D', label='DM', color='red')\n",
    "\n",
    "plt.xlabel('Number of Prototypes')\n",
    "plt.ylabel('ARI')\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/ari_dog.pdf\", bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7baf36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglop_ev=[]\n",
    "protodash_ev=[]\n",
    "dknn_ev=[]\n",
    "dmodels_ev=[]\n",
    "for i in range(len(all_protos_np)):\n",
    "    mglop_ev.append([nearest_medoid_accuracy(X_test, mod_pred, list(all_protos_np[i].values())[3]), compute_prototype_silhouette_score(X_test, list(all_protos_np[i].values())[3]), surrogate_fidelity(list(all_protos_np[i].values())[3])])\n",
    "    dmodels_ev.append([nearest_medoid_accuracy(X_test, mod_pred, list(all_protos_np[i].values())[1]), compute_prototype_silhouette_score(X_test, list(all_protos_np[i].values())[1]), surrogate_fidelity(list(all_protos_np[i].values())[1])])   \n",
    "    dknn_ev.append([nearest_medoid_accuracy(X_test, mod_pred, list(all_protos_np[i].values())[0]), compute_prototype_silhouette_score(X_test, list(all_protos_np[i].values())[0]),surrogate_fidelity(list(all_protos_np[i].values())[0])])\n",
    "    protodash_ev.append([nearest_medoid_accuracy(X_test, mod_pred, list(all_protos_np[i].values())[2]), compute_prototype_silhouette_score(X_test, list(all_protos_np[i].values())[2]), surrogate_fidelity(list(all_protos_np[i].values())[2])])\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(5,100)\n",
    "plt.plot(N_values, np.array([item[2] for item in mglop_ev]), marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, [item[2] for item in dknn_ev], marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, [item[2] for item in protodash_ev], marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, [item[2] for item in dmodels_ev], marker='D', linestyle='-', label='DM')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Faithfulness-SP\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/faithsp_dog.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(5,100)\n",
    "plt.plot(N_values, np.array([item[0] for item in mglop_ev]), marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, [item[0] for item in dknn_ev], marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, [item[0] for item in protodash_ev], marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, [item[0] for item in dmodels_ev], marker='D', linestyle='-', label='DM')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Faithfulness-NP\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/faithnp_dog.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Experiment with increasing number of prototypes\n",
    "num_prototypes_list = N_values\n",
    "avg_similarities = []\n",
    "avg_similarities1 = []\n",
    "avg_similarities2 = []\n",
    "avg_similarities3 = []\n",
    "\n",
    "for i in tqdm(range(6)):\n",
    "    mglop=list(all_protos_np[i].values())[0]\n",
    "    dmodels=list(all_protos_np[i].values())[2]\n",
    "    dknn=list(all_protos_np[i].values())[1]\n",
    "    protodash=list(all_protos_np[i].values())[3]\n",
    "    labels = cluster_by_prototypes(X_test, mglop)\n",
    "    labels1 = cluster_by_prototypes(X_test, dknn)\n",
    "    labels2 = cluster_by_prototypes(X_test, dmodels)\n",
    "    labels3 = cluster_by_prototypes(X_test, protodash)\n",
    "    avg_similarity = expected_inter_cluster_similarity(X_test, labels)\n",
    "    avg_similarity1 = expected_inter_cluster_similarity(X_test, labels1)\n",
    "    avg_similarity2 = expected_inter_cluster_similarity(X_test, labels2)\n",
    "    avg_similarity3 = expected_inter_cluster_similarity(X_test, labels3)\n",
    "    avg_similarities.append(avg_similarity)\n",
    "    avg_similarities1.append(avg_similarity1)\n",
    "    avg_similarities2.append(avg_similarity2)\n",
    "    avg_similarities3.append(avg_similarity3)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.xlim(5,100)\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.plot(num_prototypes_list, avg_similarities, marker='o', label='InfProto')\n",
    "plt.plot(num_prototypes_list, avg_similarities1, marker='s', label='KMEx')\n",
    "plt.plot(num_prototypes_list, avg_similarities3, marker='^', label='PD')\n",
    "plt.plot(num_prototypes_list, avg_similarities2, marker='D', label='DM')\n",
    "plt.xlabel('Number of Prototypes')\n",
    "plt.ylabel('Inter-Cluster Similarity')\n",
    "plt.legend(ncol=2)\n",
    "plt.savefig(\"Figures/ics_dog.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
