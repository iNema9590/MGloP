{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_752242/1235930754.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('data/model_mnist.pth'))\n",
      "/tmp/ipykernel_752242/1235930754.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dmem=torch.load('data/embedsmnist_DM.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IF.train_model_mnist import *\n",
    "from proutils import *\n",
    "import numpy as np\n",
    "import kmedoids\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from scipy.spatial.distance import cosine\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('data/model_mnist.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "ifem=np.load(\"data/influence_scores_mnist.npy\")\n",
    "dmem=torch.load('data/embedsmnist_DM.pt')\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Extract images and labels\n",
    "train_images = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])  # [N_train, 1, 28, 28]\n",
    "train_embeddings=model(train_images, emd=True).detach().numpy()\n",
    "train_labels = torch.tensor([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "test_images = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])    # [N_test, 1, 28, 28]\n",
    "test_embeddings=model(test_images, emd=True).detach().numpy()\n",
    "test_labels = torch.tensor([test_dataset[i][1] for i in range(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_fidelity(prototypes, X_test, mod_pred):\n",
    "    train_dataset = TensorDataset(X_test, mod_pred)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    smodel = fit_model(trainloader)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    smodel.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.argmax(smodel(X_test), dim=1)\n",
    "        accuracy = (outputs == mod_pred).sum().item() / len(mod_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(ifem, train_embeddings, test_embeddings, 30, coverage=True)\n",
    "prs=find_representative_samples(test_embeddings, train_embeddings, ifem, 20, 30, alpha=0.6)\n",
    "distances = cosine_similarity(test_embeddings, test_embeddings[prs])\n",
    "nearest_medoid_indices = np.argmax(distances, axis=1)\n",
    "\n",
    "# def coverage(N, bin=False, popularity=True):\n",
    "#     covs=[]\n",
    "#     for i in np.unique(nearest_medoid_indices):\n",
    "#         idx=np.where(nearest_medoid_indices == i)[0]\n",
    "#         if popularity:\n",
    "#             gx=[i[0] for i in Counter(np.array(aide_em)[idx].flatten()).most_common(60)]\n",
    "#         else:\n",
    "#             gx=aide_em[prs[i]]\n",
    "#         for j in idx:\n",
    "#             if bin:\n",
    "#                 if len(set(aide_em[j]).intersection(set(gx)))>N:\n",
    "#                     covs.append(1)\n",
    "#                 else:\n",
    "#                     covs.append(0)\n",
    "#             else:\n",
    "#                 covs.append(len(set(aide_em[j]).intersection(set(gx)))/len(aide_em[j]))\n",
    "#     return sum(covs)/len(covs)\n",
    "\n",
    "# covs= [coverage(i, bin=True, popularity=False) for i in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 60000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity to the number of influential sample to build the graph with\n",
    "mglop=Parallel(n_jobs=-1)(delayed(find_representative_samples)(test_embeddings, train_embeddings, ifem,20, i, alpha=0.6) for i in tqdm(range(5,41,5)))\n",
    "accs=[]\n",
    "fids=[]\n",
    "sil=[]\n",
    "sim=[]\n",
    "def compute_metrics(i):\n",
    "    acc = nearest_medoid_accuracy(test_embeddings, mod_pred, mglop[i])\n",
    "    fid=surrogate_fidelity(mglop[i], test_images, mod_pred)\n",
    "    sil = compute_prototype_silhouette_score(test_embeddings, mglop[i])\n",
    "    labels = cluster_by_prototypes(test_embeddings, mglop[i])\n",
    "    sim = expected_inter_cluster_similarity(test_embeddings, labels)\n",
    "    return acc,fid, sil, sim\n",
    "\n",
    "# Parallel computation\n",
    "results = Parallel(n_jobs=64)(delayed(compute_metrics)(i) for i in range(len(mglop)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs,fids, sil, sim = zip(*results)\n",
    "plt.plot(range(5,41,5), accs, marker='D', label='Faithfulness')\n",
    "plt.plot(range(5,41,5), fids, marker='*', label='Fidelity')\n",
    "plt.plot(range(5,41,5), sil, marker='x', label='Silhouette')\n",
    "plt.plot(range(5,41,5), sim, marker='o', label='Expected similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_values=range(10,51, 5)\n",
    "def generate_prototypes(N):\n",
    "    explainer = ProtodashExplainer()\n",
    "    weights, protodash, _ = explainer.explain(test_embeddings, test_embeddings, m=N, kernelType='euclid')\n",
    "    \n",
    "    return {\n",
    "        \"mglop\": find_representative_samples(test_embeddings, train_embeddings, ifem, N, 15, alpha=0.6),\n",
    "        \"dknn\": find_prototypes(test_embeddings, mod_pred, N),\n",
    "        \"dm\": find_prototypes(dmem, mod_pred, N),\n",
    "        \"protodash\": protodash\n",
    "    }\n",
    "\n",
    "n_jobs = -1 \n",
    "all_protos = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(generate_prototypes)(N)\n",
    "    for N in tqdm(N_values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection\n",
    "\n",
    "algorithms = ['mglop', 'dknn', 'dm', 'protodash']\n",
    "intersections = {f\"{algo1} ∩ {algo2}\": [] for i, algo1 in enumerate(algorithms) for algo2 in algorithms[i + 1:]}\n",
    "\n",
    "# Extract intersections between algorithm outputs\n",
    "for i, outputs in enumerate(all_protos):\n",
    "    for j, algo1 in enumerate(algorithms):\n",
    "        for algo2 in algorithms[j + 1:]:\n",
    "            intersection_size = len(set(outputs[algo1]) & set(outputs[algo2]))\n",
    "            intersections[f\"{algo1} ∩ {algo2}\"].append(intersection_size)\n",
    "\n",
    "# Plotting the results\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (label, sizes), linestyle, marker in zip(intersections.items(), line_styles, markers):\n",
    "    plt.plot(N_values, sizes, linestyle=linestyle, marker=marker, label=label)\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Intersection Size\")\n",
    "plt.title(\"Intersection Size vs Number of Elements (N) for Algorithm Outputs\")\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as a PDF\n",
    "# plt.savefig(\"Figures/intersection_plot.pdf\", format=\"pdf\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_protos_np=np.array(all_protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def assign_clusters(X, prototypes):\n",
    "\n",
    "    # Compute distance from each point to each prototype\n",
    "    distances = cdist(X, prototypes, metric='euclidean')\n",
    "    \n",
    "    # Assign each point to the cluster with minimum distance\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    return labels\n",
    "\n",
    "pairwise_aris = {\n",
    "    \"MGloP-DkNN\": [],\n",
    "    \"MGloP-DM\": [],\n",
    "    \"MGloP-PDash\": [],\n",
    "    \"DkNN-DM\": [],\n",
    "    \"DkNN-PDash\": [],\n",
    "    \"DM-PDash\": []\n",
    "}\n",
    "\n",
    "for i in range(len(all_protos_np)):\n",
    "    labels1 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[0]])\n",
    "    labels2 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[1]])\n",
    "    labels3 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[2]])\n",
    "    labels4 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[3]])\n",
    "    \n",
    "    \n",
    "    # Compute ARIs for all pairs\n",
    "    ari_12 = adjusted_rand_score(labels1, labels2)\n",
    "    ari_13 = adjusted_rand_score(labels1, labels3)\n",
    "    ari_14 = adjusted_rand_score(labels1, labels4)\n",
    "    ari_23 = adjusted_rand_score(labels2, labels3)\n",
    "    ari_24 = adjusted_rand_score(labels2, labels4)\n",
    "    ari_34 = adjusted_rand_score(labels3, labels4)\n",
    "    \n",
    "    # Append results\n",
    "    pairwise_aris[\"MGloP-DkNN\"].append(ari_12)\n",
    "    pairwise_aris[\"MGloP-DM\"].append(ari_13)\n",
    "    pairwise_aris[\"MGloP-PDash\"].append(ari_14)\n",
    "    pairwise_aris[\"DkNN-DM\"].append(ari_23)\n",
    "    pairwise_aris[\"DkNN-PDash\"].append(ari_24)\n",
    "    pairwise_aris[\"DM-PDash\"].append(ari_34)\n",
    "\n",
    "# Plot the ARI evolution for each pair\n",
    "plt.figure(figsize=(10, 6))\n",
    "for pair_name, ari_values in pairwise_aris.items():\n",
    "    plt.plot(N_values, ari_values, marker='o', label=pair_name)\n",
    "\n",
    "plt.title('ARI Evolution for Each Pair as Number of Prototypes Increases')\n",
    "plt.xlabel('Number of Prototypes (k)')\n",
    "plt.ylabel('ARI')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglop_ev=[]\n",
    "dmodels_ev=[]\n",
    "protodash_ev=[]\n",
    "dknn_ev=[]\n",
    "for i in range(len(all_protos_np)):\n",
    "    mglop_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[0]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[0]), surrogate_fidelity(list(all_protos_np[i].values())[0], test_images, mod_pred)])\n",
    "    dknn_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[1]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[1]),surrogate_fidelity(list(all_protos_np[i].values())[1], test_images, mod_pred)])\n",
    "    dmodels_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[2]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[2]), surrogate_fidelity(list(all_protos_np[i].values())[2], test_images, mod_pred)])\n",
    "    protodash_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[3]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[3]), surrogate_fidelity(list(all_protos_np[i].values())[3], test_images, mod_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range=range(10,51,5)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(N_range, [item[2] for item in mglop_ev], marker='o', linestyle='-', label='MGLop Accuracy')\n",
    "plt.plot(N_range, [item[2] for item in dknn_ev], marker='s', linestyle='--', label='DKNN Accuracy')\n",
    "plt.plot(N_range, [item[2] for item in dmodels_ev], marker='D', linestyle='-.', label='DModels Accuracy')\n",
    "plt.plot(N_range, [item[2] for item in protodash_ev], marker='^', linestyle=':', label='Protodash Accuracy')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"Surrogate Model Accuracy vs Number of Prototypes (N) for Different Algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Figures/nearest_medoid_accuracy_plot.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range=range(10,51,5)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(N_range, [item[0] for item in mglop_ev], marker='o', linestyle='-', label='MGLop Accuracy')\n",
    "plt.plot(N_range, [item[0] for item in dknn_ev], marker='s', linestyle='--', label='DKNN Accuracy')\n",
    "plt.plot(N_range, [item[0] for item in dmodels_ev], marker='D', linestyle='-.', label='DModels Accuracy')\n",
    "plt.plot(N_range, [item[0] for item in protodash_ev], marker='^', linestyle=':', label='Protodash Accuracy')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Nearest Medoid Accuracy\")\n",
    "plt.title(\"Nearest Medoid Accuracy vs Number of Prototypes (N) for Different Algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Figures/nearest_medoid_accuracy_plot.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability (Silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(N_range, [item[1] for item in mglop_ev], marker='o', linestyle='-', label='MGLop Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in dknn_ev], marker='s', linestyle='--', label='DKNN Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in dmodels_ev], marker='D', linestyle='-.', label='DModels Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in protodash_ev], marker='^', linestyle=':', label='Protodash Silhouette Score')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Prototypes (N) for Different Algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Figures/silhouette_score_plot.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Inter-cluster similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Experiment with increasing number of prototypes\n",
    "num_prototypes_list = range(10, 51, 5)\n",
    "avg_similarities = []\n",
    "avg_similarities1 = []\n",
    "avg_similarities2 = []\n",
    "avg_similarities3 = []\n",
    "\n",
    "for i in tqdm(range(9)):\n",
    "    mglop=list(all_protos_np[i].values())[0]\n",
    "    dmodels=list(all_protos_np[i].values())[2]\n",
    "    dknn=list(all_protos_np[i].values())[1]\n",
    "    protodash=list(all_protos_np[i].values())[3]\n",
    "    labels = cluster_by_prototypes(test_embeddings, mglop)\n",
    "    labels1 = cluster_by_prototypes(test_embeddings, dknn)\n",
    "    labels2 = cluster_by_prototypes(test_embeddings, dmodels)\n",
    "    labels3 = cluster_by_prototypes(test_embeddings, protodash)\n",
    "    avg_similarity = expected_inter_cluster_similarity(test_embeddings, labels)\n",
    "    avg_similarity1 = expected_inter_cluster_similarity(test_embeddings, labels1)\n",
    "    avg_similarity2 = expected_inter_cluster_similarity(test_embeddings, labels2)\n",
    "    avg_similarity3 = expected_inter_cluster_similarity(test_embeddings, labels3)\n",
    "    avg_similarities.append(avg_similarity)\n",
    "    avg_similarities1.append(avg_similarity1)\n",
    "    avg_similarities2.append(avg_similarity2)\n",
    "    avg_similarities3.append(avg_similarity3)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_prototypes_list, avg_similarities, marker='o', label='MGloP')\n",
    "plt.plot(num_prototypes_list, avg_similarities1, marker='x', label='DkNN')\n",
    "plt.plot(num_prototypes_list, avg_similarities2, marker='*', label='Dmodels')\n",
    "plt.plot(num_prototypes_list, avg_similarities3, marker='D', label='ProtoDash')\n",
    "plt.xlabel('Number of Prototypes')\n",
    "plt.ylabel('Expected Inter-Cluster Similarity')\n",
    "plt.title('Expected Inter-Cluster Similarity vs Number of Prototypes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
