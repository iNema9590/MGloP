{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710589/4040735843.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('data/main_model.pth'))\n",
      "/tmp/ipykernel_710589/4040735843.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_embeddings = torch.load('data/train_embeddings.pt')\n",
      "/tmp/ipykernel_710589/4040735843.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_labels = torch.load('data/train_labels.pt')\n",
      "/tmp/ipykernel_710589/4040735843.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_embeddings = torch.load('data/test_embeddings.pt')\n",
      "/tmp/ipykernel_710589/4040735843.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_labels = torch.load('data/test_labels.pt')\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from IF import *\n",
    "from proutils import *\n",
    "import pandas as pd\n",
    "model = LinearModel()\n",
    "model.load_state_dict(torch.load('data/main_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "ifem=np.load(\"data/influence_scores.npy\")\n",
    "train_embeddings = torch.load('data/train_embeddings.pt')\n",
    "train_labels = torch.load('data/train_labels.pt')\n",
    "test_embeddings = torch.load('data/test_embeddings.pt')\n",
    "test_labels = torch.load('data/test_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aide(inf, train_embeddings, test_embeddings):\n",
    "\n",
    "    aide_emb = []\n",
    "    for i, emb in enumerate(inf):\n",
    "        # Sort indices based on embedding values\n",
    "        ind = emb.argsort()\n",
    "        \n",
    "        # Top 10 positive and negative indices\n",
    "        top_pos_indices = ind[-10:]\n",
    "        top_neg_indices = ind[:10]\n",
    "\n",
    "        # Compute cosine similarity for positive and negative indices\n",
    "        pos_sim = cosine_similarity(train_embeddings[top_pos_indices], test_embeddings[i].reshape(1, -1)).flatten()\n",
    "        neg_sim = cosine_similarity(train_embeddings[top_neg_indices], test_embeddings[i].reshape(1, -1)).flatten()\n",
    "\n",
    "        # Select top 5 positive and negative indices based on cosine similarity\n",
    "        pos_idx = top_pos_indices[pos_sim.argsort()[-5:]]\n",
    "        neg_idx = top_neg_indices[neg_sim.argsort()[-5:]]\n",
    "\n",
    "        # Combine positive and negative indices\n",
    "        aide_emb.append(np.append(pos_idx,neg_idx))\n",
    "\n",
    "    return aide_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(ifem, train_embeddings, test_embeddings, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i, idx in enumerate(aide_em):\n",
    "    G.add_node(i, bipartite=0)\n",
    "    for j in idx:\n",
    "        G.add_node(f'ex-{j}', bipartite=1)\n",
    "        G.add_edge(i, f'ex-{j}', weight=0.5)\n",
    "# nx.write_gml(G, \"data/bigraph_aide.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(ifem, train_embeddings, test_embeddings, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G[0]['ex-1227']['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops=list(map(lambda x: int(x.split('-')[1]), top_k_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "degrees=[G.degree(i) for i in G.nodes if G.nodes[i]['bipartite']==1]\n",
    "plt.hist(degrees, bins=130)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ticklabel_format(style='sci', axis='x')\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"Centrality Degree\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "# plt.savefig('inf_cnn_sortsum.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_0 = [n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0]\n",
    "test_embeddings[nodes_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def find_representative_samples(bipartite_graph, bipartite=0):\n",
    "\n",
    "    # Extract nodes and neighbors\n",
    "    nodes_0 = [n for n, d in bipartite_graph.nodes(data=True) if d[\"bipartite\"] == bipartite]\n",
    "    neighbors = {node: set(bipartite_graph.neighbors(node)) for node in nodes_0}\n",
    "    \n",
    "    # Compute pairwise connection strengths\n",
    "    num_nodes = len(nodes_0)\n",
    "    connection_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    \n",
    "    for i, node1 in enumerate(nodes_0):\n",
    "        for j, node2 in enumerate(nodes_0):\n",
    "            if i != j:\n",
    "                # Compute shared neighbors count\n",
    "                connection_matrix[i, j] = len(neighbors[node1] & neighbors[node2])\n",
    "    \n",
    "    # Compute semantic similarity\n",
    "    similarity_matrix = cosine_similarity(test_embeddings[nodes_0])\n",
    "    \n",
    "    # Combine connection strength and semantic similarity (prioritizing connections)\n",
    "    combined_matrix = connection_matrix + 0.5 * similarity_matrix\n",
    "    \n",
    "    # Perform clustering\n",
    "    clustering = SpectralClustering(\n",
    "        n_clusters=20,  \n",
    "        affinity=\"precomputed\",\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = clustering.fit_predict(combined_matrix)\n",
    "    \n",
    "    # Select representatives\n",
    "    representatives = []\n",
    "    for cluster_id in np.unique(labels):\n",
    "        cluster_nodes = [nodes_0[i] for i in range(num_nodes) if labels[i] == cluster_id]\n",
    "        # Select the node with the maximum connections\n",
    "        representative = max(cluster_nodes, key=lambda n: sum(connection_matrix[nodes_0.index(n)]))\n",
    "        representatives.append(representative)\n",
    "    \n",
    "    return representatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protos=find_representative_samples(G, bipartite=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[[2866,\n",
    " 7256,\n",
    " 2715,\n",
    " 299,\n",
    " 9000,\n",
    " 3438,\n",
    " 6279,\n",
    " 9554,\n",
    " 8379,\n",
    " 9908,\n",
    " 291,\n",
    " 7721,\n",
    " 34,\n",
    " 3717,\n",
    " 7888,\n",
    " 8565,\n",
    " 4415,\n",
    " 871,\n",
    " 4160,\n",
    " 9136]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def display_cifar10_images(indices, train=False):\n",
    "    \"\"\"\n",
    "    Display CIFAR-10 images based on a list of indices and train/test flag.\n",
    "    \n",
    "    Args:\n",
    "        indices (list of int): List of indices to display.\n",
    "        train (bool): If True, load images from the training set. If False, use the test set.\n",
    "    \"\"\"\n",
    "    # Define transformations for the dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR-10 dataset (train or test)\n",
    "    cifar10 = datasets.CIFAR10(root='./data', train=train, download=True, transform=transform)\n",
    "    \n",
    "    # Classes in CIFAR-10\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Number of images to display\n",
    "    num_images = len(indices)\n",
    "    \n",
    "    # Set up a grid for the images\n",
    "    plt.figure(figsize=(12, num_images * 2))  # Adjust figure size based on the number of images\n",
    "    for i, index in enumerate(indices):\n",
    "        image, label = cifar10[index]  # Get image and label at the given index\n",
    "        \n",
    "        # Resize the image for better display quality\n",
    "        resized_image = F.resize(image, size=(128, 128))  # Resize to 128x128\n",
    "        \n",
    "        # Convert tensor to numpy format for visualization\n",
    "        resized_image = resized_image.permute(1, 2, 0)  # Rearrange channels for matplotlib\n",
    "        \n",
    "        # Plot each image in a grid\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(resized_image)\n",
    "        plt.title(f\"{classes[label]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "indices_to_display = [2] # List of indices to display\n",
    "display_cifar10_images(indices_to_display, train=False)  # Display test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
