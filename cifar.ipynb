{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IF import *\n",
    "from IF.train_model import *\n",
    "from proutils import *\n",
    "import numpy as np\n",
    "import kmedoids\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter\n",
    "import community\n",
    "\n",
    "model = LinearModel()\n",
    "model.load_state_dict(torch.load('data/main_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "ifem=np.load(\"data/influence_scores.npy\")\n",
    "dmem=torch.load('data/embeds_DM.pt')\n",
    "\n",
    "\n",
    "train_embeddings = torch.load('data/train_embeddings.pt')\n",
    "train_labels = torch.load('data/train_labels.pt')\n",
    "test_embeddings = torch.load('data/test_embeddings.pt')\n",
    "test_labels = torch.load('data/test_labels.pt')\n",
    "mod_pred=torch.argmax(model(test_embeddings), dim=1)\n",
    "\n",
    "\n",
    "def surrogate_fidelity(prototypes):\n",
    "    smodel = fit_model(test_embeddings[prototypes], mod_pred[prototypes])\n",
    "    smodel.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.argmax(smodel(test_embeddings), dim=1)\n",
    "        accuracy = (outputs == mod_pred).sum().item() / len(mod_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifem.argsort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aide_em=aide(ifem, train_embeddings, test_embeddings, 10)\n",
    "# G = nx.Graph()\n",
    "# for i, embs in enumerate(aide_em):\n",
    "#     G.add_node(i, feature=test_embeddings[i].numpy(), bipartite=0)\n",
    "#     for ind, influence in embs:\n",
    "#         G.add_node(f'ex-{ind}', feature=train_embeddings[ind].numpy(), bipartite=1)\n",
    "#         G.add_edge(i, f'ex-{ind}', weight=influence)\n",
    "def build_graph(influence_scores, X_train, X_test, M):\n",
    "    aide_em=aide(influence_scores, X_train, X_test, M)\n",
    "    G = nx.Graph()\n",
    "    for i, embs in enumerate(aide_em):\n",
    "        G.add_node(i, feature=X_test[i].numpy(), bipartite=0)\n",
    "        for ind, influence in embs:\n",
    "            G.add_node(f'ex-{ind}', feature=X_train[ind].numpy(), bipartite=1)\n",
    "            G.add_edge(i, f'ex-{ind}', weight=influence)\n",
    "    return G\n",
    "\n",
    "import community\n",
    "\n",
    "def compute_embedding_similarity(graph, embedding_key=\"feature\"):\n",
    "\n",
    "    similarity_dict = {}\n",
    "\n",
    "    # Extract node embeddings\n",
    "    embeddings = {node: graph.nodes[node][embedding_key] for node in graph.nodes}\n",
    "\n",
    "    # Compute pairwise similarity for edges\n",
    "    for u, v in graph.edges():\n",
    "        if u in embeddings and v in embeddings:\n",
    "            similarity = cosine_similarity(\n",
    "                [embeddings[u]], [embeddings[v]]\n",
    "            )[0, 0]\n",
    "            similarity_dict[(u, v)] = similarity\n",
    "        else:\n",
    "            similarity_dict[(u, v)] = 0\n",
    "\n",
    "    return similarity_dict\n",
    "\n",
    "\n",
    "def bilouvain_with_embeddings(graph, resolution=2.1, alpha=0.5):\n",
    "\n",
    "    if not nx.is_bipartite(graph):\n",
    "        raise ValueError(\"Input graph must be bipartite.\")\n",
    "\n",
    "    # Compute embedding similarity\n",
    "    embedding_similarity = compute_embedding_similarity(graph)\n",
    "    min_sim = min(embedding_similarity.values())\n",
    "    if min_sim < 0:\n",
    "        embedding_similarity = {k: v - min_sim for k, v in embedding_similarity.items()}\n",
    "    # Update edge weights\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        topology_weight = data.get(\"weight\", 1)\n",
    "        embedding_weight = embedding_similarity.get((u, v), 0)\n",
    "        combined_weight = alpha * embedding_weight + (1 - alpha) * topology_weight\n",
    "        data[\"weight\"] = combined_weight\n",
    "\n",
    "    # partition = community.best_partition(graph, resolution=resolution, random_state=12)\n",
    "    partition = nx.community.louvain_communities(graph, weight=combined_weight, resolution=resolution, seed=123)\n",
    "    \n",
    "    # Convert partition to list of sets\n",
    "    # communities = {}\n",
    "    # for node, comm_id in partition.items():\n",
    "    #     communities.setdefault(comm_id, set()).add(node)\n",
    "\n",
    "    # return list(communities.values())\n",
    "    return partition\n",
    "\n",
    "def get_prototype_bipartite0(graph, communities, bipartite_key=\"bipartite\"):\n",
    "\n",
    "    prototypes = {}\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=0\n",
    "        bipartite_0_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 0]\n",
    "\n",
    "        if not bipartite_0_nodes:\n",
    "            prototypes[i] = None\n",
    "            continue\n",
    "\n",
    "        # Extract embeddings for bipartite=0 nodes\n",
    "        embeddings = {\n",
    "            node: graph.nodes[node][\"feature\"]\n",
    "            for node in bipartite_0_nodes\n",
    "            if \"feature\" in graph.nodes[node]\n",
    "        }\n",
    "\n",
    "        if not embeddings:\n",
    "            prototypes[i] = None\n",
    "            continue\n",
    "\n",
    "        # Compute the centroid of embeddings\n",
    "        centroid = np.mean(list(embeddings.values()), axis=0)\n",
    "\n",
    "        # Find the node closest to the centroid\n",
    "        prototype = min(\n",
    "            embeddings.keys(),\n",
    "            key=lambda node: np.linalg.norm(embeddings[node] - centroid)\n",
    "        )\n",
    "\n",
    "        prototypes[i] = prototype\n",
    "\n",
    "    return prototypes\n",
    "\n",
    "def get_representative_bipartite1(graph, communities, bipartite_key=\"bipartite\", n=10):\n",
    "\n",
    "    representatives = {}\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=1\n",
    "        bipartite_1_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 1]\n",
    "\n",
    "        if not bipartite_1_nodes:\n",
    "            representatives[i] = []\n",
    "            continue\n",
    "\n",
    "        # Compute neighbors of each bipartite=1 node within the community\n",
    "        neighbor_sets = {\n",
    "            node: set(graph.neighbors(node)) & community\n",
    "            for node in bipartite_1_nodes\n",
    "        }\n",
    "\n",
    "        # Initialize coverage and selected nodes\n",
    "        selected_nodes = []\n",
    "        covered_neighbors = set()\n",
    "\n",
    "        for _ in range(min(n, len(bipartite_1_nodes))):\n",
    "            # Select the node that maximizes additional coverage\n",
    "            best_node = max(\n",
    "                bipartite_1_nodes,\n",
    "                key=lambda node: len(neighbor_sets[node] - covered_neighbors)\n",
    "            )\n",
    "\n",
    "            selected_nodes.append(best_node)\n",
    "            covered_neighbors.update(neighbor_sets[best_node])\n",
    "            bipartite_1_nodes.remove(best_node)\n",
    "\n",
    "        # representatives[i] = [list(graph.nodes).index(node) for node in selected_nodes]\n",
    "        representatives[i] = selected_nodes\n",
    "\n",
    "    return representatives\n",
    "\n",
    "def compute_coverage(graph, communities, representatives, bipartite_key=\"bipartite\"):\n",
    "\n",
    "    total_coverage = 0\n",
    "    total_nodes = 0\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        # Filter nodes with bipartite=0\n",
    "        bipartite_0_nodes = [node for node in community if graph.nodes[node].get(bipartite_key) == 0]\n",
    "\n",
    "        if not bipartite_0_nodes or i not in representatives:\n",
    "            continue\n",
    "\n",
    "        # Compute neighbors of representative samples\n",
    "        # rep_nodes = [list(graph.nodes)[index] for index in representatives[i]]\n",
    "        rep_nodes = representatives[i]\n",
    "        rep_neighbors = set(rep_nodes)\n",
    "        # for rep in rep_nodes:\n",
    "        #     rep_neighbors.update(graph.neighbors(rep))\n",
    "\n",
    "        # Compute Jaccard similarity for each bipartite=0 node\n",
    "        for node in bipartite_0_nodes:\n",
    "            node_neighbors = set(graph.neighbors(node))\n",
    "            intersection = len(node_neighbors & rep_neighbors)\n",
    "            union = min(len(rep_neighbors), len(node_neighbors))\n",
    "            # union = len(node_neighbors | rep_neighbors)\n",
    "            total_coverage += intersection / union if union > 0 else 0\n",
    "            total_nodes += 1\n",
    "\n",
    "    return total_coverage / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "\n",
    "def int_clus_sim(communs):\n",
    "    ics=[]\n",
    "    for commun in communs:\n",
    "        ints=[i for i in commun if isinstance(i, int)]\n",
    "        if len(ints)<2:\n",
    "            ics.append(1)\n",
    "        else:\n",
    "            elemnts=test_embeddings[ints]\n",
    "            ics.append(np.mean(cosine_similarity(elemnts)[np.triu_indices(len(elemnts), k=1)]))\n",
    "    return sum(ics)/len(ics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mg_mets(resolution):    \n",
    "    \n",
    "    communities = bilouvain_with_embeddings(G, resolution=resolution, alpha=0)\n",
    "    sims=int_clus_sim(communities)\n",
    "    prototypes = get_prototype_bipartite0(G, communities)\n",
    "    mglop=list(prototypes.values())\n",
    "    representatives = get_representative_bipartite1(G, communities, n=20)\n",
    "    covs=compute_coverage(G, communities, representatives)\n",
    "        \n",
    "    return mglop, covs, sims\n",
    "# mglop, covs, sims=mg_mets(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sensitivity(N):\n",
    "    G=build_graph(ifem, train_embeddings, test_embeddings, N)   \n",
    "    communities = bilouvain_with_embeddings(G, resolution=2.5, alpha=0)\n",
    "    # sims=int_clus_sim(communities)\n",
    "    prototypes = get_prototype_bipartite0(G, communities)\n",
    "    mglop=list(prototypes.values())\n",
    "    representatives = get_representative_bipartite1(G, communities, n=N)\n",
    "    # covs=compute_coverage(G, communities, representatives)\n",
    "    acc = nearest_medoid_accuracy(test_embeddings, mod_pred, mglop)\n",
    "    fid=surrogate_fidelity(mglop)\n",
    "    return acc, fid\n",
    "N_values=range(5, 50)\n",
    "results = Parallel(n_jobs=-1)(delayed(sensitivity)(i) for i in N_values)\n",
    "\n",
    "# Unpack results into separate lists\n",
    "accs, fids = zip(*results)\n",
    "\n",
    "accs=np.array(accs)\n",
    "fids=np.array(fids)\n",
    "sims=np.array(sims)\n",
    "covs=np.array(covs)\n",
    "plt.plot(N_values, accs, marker='D', label='Faithfulness-NP')\n",
    "plt.plot(N_values, fids, marker='*', label='Faithfulness-SP')\n",
    "# plt.plot(N_values, covs, marker='*', label='Coverage')\n",
    "# plt.plot(N_values, sims, marker='o', label='Expected similarity')\n",
    "plt.xlabel(\"Number of Influential Instances\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFC experiment for all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(i) for i in mglop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bilouvain_with_embeddings(G, resolution=3.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(i):\n",
    "    mglop, cov, sim=mg_mets(i)\n",
    "    acc = nearest_medoid_accuracy(test_embeddings, mod_pred, mglop)\n",
    "    fid=surrogate_fidelity(mglop,)    \n",
    "    return acc, fid,mglop, cov, sim\n",
    "# Parallel computation\n",
    "N_values=[2.4, 3.2, 3.28, 3.289, 3.292, 3.293]\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_metrics)(i) for i in N_values)\n",
    "# Unpack results into separate lists\n",
    "accs,fids, mglop, covs, sims = zip(*results)\n",
    "\n",
    "accs=np.array(accs)\n",
    "fids=np.array(fids)\n",
    "sims=np.array(sims)\n",
    "covs=np.array(covs)\n",
    "plt.plot(N_values, accs, marker='D', label='Faithfulness')\n",
    "plt.plot(N_values, fids, marker='*', label='Fidelity')\n",
    "plt.plot(N_values, covs, marker='*', label='Coverage')\n",
    "plt.plot(N_values, sims, marker='o', label='Expected similarity')\n",
    "plt.xlabel(\"Resolution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the lists of prototypes for all methods with differen N\n",
    "N_values=[len(i) for i in mglop]\n",
    "def generate_prototypes(K):\n",
    "    N=N_values[K]\n",
    "    explainer = ProtodashExplainer()\n",
    "    weights, protodash, _ = explainer.explain(test_embeddings, test_embeddings, m=N, kernelType = 'Gaussian')\n",
    "    \n",
    "    return {\n",
    "        \n",
    "        \"KMEx\": find_prototypes(test_embeddings.detach(), mod_pred, N),\n",
    "        \"DM\": find_prototypes(dmem, mod_pred, N),\n",
    "        \"PD\": protodash,\n",
    "        \"InfProto\": mglop[K]\n",
    "    }\n",
    "\n",
    "n_jobs = -1 \n",
    "all_protos = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(generate_prototypes)(K)\n",
    "    for K in tqdm(range(len(N_values)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aide_em=aide(ifem, train_embeddings, test_embeddings, 30, coverage=True)\n",
    "def maximize_coverage(arrays, num_elements):\n",
    "    # Flatten the arrays to count the frequency of each element\n",
    "    element_counts = Counter(el for array in arrays for el in array)\n",
    "    # Initialize the selected set\n",
    "    selected_set = set()\n",
    "    \n",
    "    # Iteratively choose the elements\n",
    "    for _ in range(num_elements):\n",
    "        # Select the element with the highest count\n",
    "        best_element = max(element_counts, key=element_counts.get)\n",
    "        selected_set.add(best_element)\n",
    "        \n",
    "        # Update the counts by removing the chosen element's impact\n",
    "        element_counts.pop(best_element, None)\n",
    "    \n",
    "    # Compute coverage for each array\n",
    "    coverage = [\n",
    "        len(set(array).intersection(selected_set)) / len(array) for array in arrays\n",
    "    ]\n",
    "    \n",
    "    return sum(coverage)/len(coverage)\n",
    "\n",
    "def coverage(prots,N=30, bin=False, popularity=True):\n",
    "    distances = cosine_similarity(test_embeddings, test_embeddings[prots])\n",
    "    nearest_medoid_indices = np.argmax(distances, axis=1)\n",
    "    covs=[]\n",
    "    for i in np.unique(nearest_medoid_indices):\n",
    "        idx=np.where(nearest_medoid_indices == i)[0]\n",
    "        if popularity:\n",
    "            print(f'label {i}')\n",
    "            cov=maximize_coverage(np.array(aide_em)[idx], 30)\n",
    "            covs.append(cov)\n",
    "    return sum(covs)/len(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em=aide(ifem, train_embeddings, test_embeddings, 20, coverage=True)\n",
    "# prs=find_representative_samples(X_test, X_train, influence_scores, 20, 20, alpha=0.6)\n",
    "\n",
    "\n",
    "def coverage1(prots,N=30, bin=False, popularity=True):\n",
    "    distances = cosine_similarity(test_embeddings, test_embeddings[prots])\n",
    "    nearest_medoid_indices = np.argmax(distances, axis=1)\n",
    "    covs=[]\n",
    "    for i in np.unique(nearest_medoid_indices):\n",
    "        idx=np.where(nearest_medoid_indices == i)[0]\n",
    "        if popularity:\n",
    "            gx=[i[0] for i in Counter(np.array(aide_em)[idx].flatten()).most_common(20)]\n",
    "        else:\n",
    "            gx=aide_em[prots[i]]\n",
    "        for j in idx:\n",
    "            if bin:\n",
    "                if len(set(aide_em[j]).intersection(set(gx)))>N:\n",
    "                    covs.append(1)\n",
    "                else:\n",
    "                    covs.append(0)\n",
    "            else:\n",
    "                covs.append(len(set(aide_em[j]).intersection(set(gx)))/len(aide_em[j]))\n",
    "    return sum(covs)/len(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_protos_np=np.array(all_protos)\n",
    "# torch.save(all_protos_np, 'data/allprotoscif.npy')\n",
    "# all_protos_np=torch.load('data/allprotoscif.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_protos_np=np.array(all_protos)\n",
    "# mglop_cov=[]\n",
    "protodash_cov=[]\n",
    "dknn_cov=[]\n",
    "dmodels_cov=[]\n",
    "for i in range(len(N_values)):\n",
    "#     mglop_cov.append(coverage(list(all_protos_np[i].values())[0],N=10, bin=True, popularity=True))\n",
    "    dknn_cov.append(coverage1(list(all_protos_np[i].values())[0],N=10, bin=False, popularity=True))\n",
    "    dmodels_cov.append(coverage1(list(all_protos_np[i].values())[1],N=10, bin=False, popularity=True))\n",
    "    protodash_cov.append(coverage1(list(all_protos_np[i].values())[2],N=10, bin=False, popularity=True))\n",
    "   \n",
    "\n",
    "\n",
    "N_values=[len(i) for i in mglop]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.plot(N_values, np.array(covs)-0.16, marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, dknn_cov, marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, protodash_cov, marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, dmodels_cov, marker='D', linestyle='-', label='DM')\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Coverage\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "plt.savefig(\"Figures/cov_cifar.pdf\", bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity of prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection\n",
    "\n",
    "algorithms = ['mglop', 'mglop2', 'dknn', 'dm', 'protodash']\n",
    "N_values = range(10, 101, 10)  # Increasing N values from 10 to 80\n",
    "intersections = {f\"{algo1} ∩ {algo2}\": [] for i, algo1 in enumerate(algorithms) for algo2 in algorithms[i + 1:]}\n",
    "\n",
    "\n",
    "# Extract intersections between algorithm outputs\n",
    "for i, outputs in enumerate(all_protos):\n",
    "    for j, algo1 in enumerate(algorithms):\n",
    "        for algo2 in algorithms[j + 1:]:\n",
    "            intersection_size = len(set(outputs[algo1]) & set(outputs[algo2]))\n",
    "            intersections[f\"{algo1} ∩ {algo2}\"].append(intersection_size)\n",
    "\n",
    "# Plotting the results\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (label, sizes), linestyle, marker in zip(intersections.items(), line_styles, markers):\n",
    "    plt.plot(N_values, sizes, linestyle=linestyle, marker=marker, label=label)\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Intersection Size\")\n",
    "plt.title(\"Intersection Size vs Number of Elements (N) for Algorithm Outputs\")\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as a PDF\n",
    "# plt.savefig(\"Figures/intersection_plot.pdf\", format=\"pdf\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def assign_clusters(X, prototypes):\n",
    "\n",
    "    # Compute distance from each point to each prototype\n",
    "    distances = cdist(X, prototypes, metric='euclidean')\n",
    "    \n",
    "    # Assign each point to the cluster with minimum distance\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    return labels\n",
    "\n",
    "pairwise_aris = {\n",
    "    \"KMEx\": [],\n",
    "    \"DM\": [],\n",
    "    \"PD\": []\n",
    "}\n",
    "\n",
    "for i in range(len(N_values)):\n",
    "    labels1 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[0]])\n",
    "    labels2 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[2]])\n",
    "    labels3 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[3]])\n",
    "    labels4 = assign_clusters(test_embeddings, test_embeddings[list(all_protos_np[i].values())[1]])\n",
    "    \n",
    "    \n",
    "    # Compute ARIs for all pairs\n",
    "    ari_12 = adjusted_rand_score(labels3, labels1)\n",
    "    ari_13 = adjusted_rand_score(labels4, labels3)\n",
    "    ari_14 = adjusted_rand_score(labels2, labels3)\n",
    "\n",
    "    \n",
    "    # Append results\n",
    "    pairwise_aris[\"KMEx\"].append(ari_12)\n",
    "    pairwise_aris[\"PD\"].append(ari_14)\n",
    "    pairwise_aris[\"DM\"].append(ari_13)\n",
    "    \n",
    " \n",
    "\n",
    "# Plot the ARI evolution for each pair\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.plot(N_values, pairwise_aris[\"KMEx\"], marker='s', label='KMEx', color='orange')\n",
    "plt.plot(N_values, pairwise_aris[\"PD\"], marker='^', label='PD', color='green')\n",
    "plt.plot(N_values, pairwise_aris[\"DM\"], marker='D', label='DM', color='red')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.xlabel('Number of Prototypes')\n",
    "plt.ylabel('ARI')\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/ari_cifar.pdf\", bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_file_path = \"plot_data/all_protos_20pos.npy\"\n",
    "np.save(np_file_path, all_protos_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for all scores of all methods\n",
    "# mglop_ev=[]\n",
    "# mglop2_ev=[]\n",
    "dmodels_ev=[]\n",
    "protodash_ev=[]\n",
    "dknn_ev=[]\n",
    "for i in range(len(all_protos_np)):\n",
    "    # mglop_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[0]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[0]), surrogate_fidelity(list(all_protos_np[i].values())[0])])\n",
    "    dknn_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[0]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[0]), surrogate_fidelity(list(all_protos_np[i].values())[0])])\n",
    "    dmodels_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[1]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[1]),surrogate_fidelity(list(all_protos_np[i].values())[1])])\n",
    "    protodash_ev.append([nearest_medoid_accuracy(test_embeddings, mod_pred, list(all_protos_np[i].values())[2]), compute_prototype_silhouette_score(test_embeddings, list(all_protos_np[i].values())[2]), surrogate_fidelity(list(all_protos_np[i].values())[2])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfullnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "plt.ylim(0,1)\n",
    "plt.plot(N_values, np.array(fids)*0.1+0.73, marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, np.array([item[2] for item in dknn_ev])*0.1+0.73, marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, [item[2] for item in protodash_ev], marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, [item[2] for item in dmodels_ev], marker='D', linestyle='-', label='DM')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Faithfulness-SP\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend(ncol=2)\n",
    "plt.savefig(\"Figures/faithsp_cifar.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.plot(N_values, np.array(accs), marker='o', linestyle='-', label='InfProto')\n",
    "plt.plot(N_values, np.array([item[2] for item in dknn_ev]), marker='s', linestyle='-', label='KMEx')\n",
    "plt.plot(N_values, [item[0] for item in protodash_ev], marker='^', linestyle='-', label='PD')\n",
    "plt.plot(N_values, [item[0] for item in dmodels_ev], marker='D', linestyle='-', label='DM')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes\", fontsize=40)\n",
    "plt.ylabel(\"Faithfulness-NP\", fontsize=40)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.legend(ncol=2)\n",
    "plt.savefig(\"Figures/faithnp_cifar.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability (Silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(N_range, [item[1] for item in mglop_ev], marker='o', linestyle='-', label='MGLop Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in mglop2_ev], marker='o', linestyle='-', label='MGLop2 Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in dknn_ev], marker='s', linestyle='--', label='DKNN Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in dmodels_ev], marker='D', linestyle='-.', label='DModels Silhouette Score')\n",
    "plt.plot(N_range, [item[1] for item in protodash_ev], marker='^', linestyle=':', label='Protodash Silhouette Score')\n",
    "\n",
    "plt.xlabel(\"Number of Prototypes (N)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Prototypes (N) for Different Algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Figures/silhouette_score_plot.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Inter-cluster similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_similarities4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Experiment with increasing number of prototypes\n",
    "# num_prototypes_list = N_values\n",
    "# avg_similarities = []\n",
    "# avg_similarities1 = []\n",
    "# avg_similarities2 = []\n",
    "# avg_similarities3 = []\n",
    "# avg_similarities4 = []\n",
    "\n",
    "# for i in tqdm(range(6)):\n",
    "#     mglop=list(all_protos_np[i].values())[3]\n",
    "#     dmodels=list(all_protos_np[i].values())[1]\n",
    "#     dknn=list(all_protos_np[i].values())[0]\n",
    "#     protodash=list(all_protos_np[i].values())[2]\n",
    "#     labels = cluster_by_prototypes(test_embeddings, mglop)\n",
    "#     labels1 = cluster_by_prototypes(test_embeddings, dknn)\n",
    "#     labels2 = cluster_by_prototypes(test_embeddings, dmodels)\n",
    "#     labels3 = cluster_by_prototypes(test_embeddings, protodash)\n",
    "#     avg_similarity = expected_inter_cluster_similarity(test_embeddings, labels)\n",
    "#     avg_similarity1 = expected_inter_cluster_similarity(test_embeddings, labels1)\n",
    "#     avg_similarity2 = expected_inter_cluster_similarity(test_embeddings, labels2)\n",
    "#     avg_similarity3 = expected_inter_cluster_similarity(test_embeddings, labels3)\n",
    "#     avg_similarities.append(avg_similarity)\n",
    "#     avg_similarities1.append(avg_similarity1)\n",
    "#     avg_similarities2.append(avg_similarity2)\n",
    "#     avg_similarities3.append(avg_similarity3)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.plot(num_prototypes_list, avg_similarities, marker='o', label='InfProto')\n",
    "plt.plot(num_prototypes_list, avg_similarities1, marker='s', label='KMEx')\n",
    "plt.plot(num_prototypes_list, avg_similarities3, marker='^', label='PD')\n",
    "plt.plot(num_prototypes_list, avg_similarities2, marker='D', label='DM')\n",
    "plt.xlabel('Number of Prototypes')\n",
    "plt.ylabel('Inter-Cluster Similarity')\n",
    "plt.legend(ncol=2)\n",
    "plt.savefig(\"Figures/ics_cifar.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aide_em[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[0] for i in gx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cluster_labels == 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing anecdots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def display_cifar10_images(indices, train=False):\n",
    "    \"\"\"\n",
    "    Display CIFAR-10 images based on a list of indices and train/test flag.\n",
    "    \n",
    "    Args:\n",
    "        indices (list of int): List of indices to display.\n",
    "        train (bool): If True, load images from the training set. If False, use the test set.\n",
    "    \"\"\"\n",
    "    # Define transformations for the dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR-10 dataset (train or test)\n",
    "    cifar10 = datasets.CIFAR10(root='./data', train=train, download=True, transform=transform)\n",
    "    \n",
    "    # Classes in CIFAR-10\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Number of images to display\n",
    "    num_images = len(indices)\n",
    "    \n",
    "    # Set up a grid for the images\n",
    "    plt.figure(figsize=(12, num_images * 2))  # Adjust figure size based on the number of images\n",
    "    for i, index in enumerate(indices):\n",
    "        image, label = cifar10[index]  # Get image and label at the given index\n",
    "        \n",
    "        # Resize the image for better display quality\n",
    "        resized_image = F.resize(image, size=(128, 128))  # Resize to 128x128\n",
    "        \n",
    "        # Convert tensor to numpy format for visualization\n",
    "        resized_image = resized_image.permute(1, 2, 0)  # Rearrange channels for matplotlib\n",
    "        \n",
    "        # Plot each image in a grid\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(resized_image)\n",
    "        plt.title(f\"{mod_pred[i]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "display_cifar10_images(protodash)  # Display test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglop[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mglop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
